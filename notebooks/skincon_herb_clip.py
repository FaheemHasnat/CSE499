# -*- coding: utf-8 -*-
"""skincon_herb_clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PNlS06t8nKJj_kzfxLU8ZaYybTShZMdw

# CLIP-Based Plant Compound to Skin Condition Matching

**Objective**: Use CLIP model to match plant compound descriptions with skin condition images

**Project**: Vision-Language Model (VLM) for predicting plant compound effects on human skin

**Data Sources**:
1. `herb2_final_clean.csv` - Plant compound information (text)
2. `skincon_preprocessed.csv` - Skin condition data with image filenames
3. Skin images: `/content/drive/MyDrive/Skincon/preprocessed/images/`

**Model**: OpenAI CLIP (clip-vit-base-patch32)

## 1. Import Required Libraries
"""

# Core data processing libraries
import pandas as pd
import numpy as np
import os
from pathlib import Path
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

# Image processing
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Deep learning libraries
import torch
from transformers import CLIPProcessor, CLIPModel
from torch.nn.functional import cosine_similarity

# Progress bar
from tqdm.auto import tqdm

# Google Drive mounting (for Colab)
try:
    from google.colab import drive
    IN_COLAB = True
    print("Running in Google Colab")
except ImportError:
    IN_COLAB = False
    print("Running in local environment")

print(f"\nLibrary versions:")
print(f"  - Pandas: {pd.__version__}")
print(f"  - NumPy: {np.__version__}")
print(f"  - PyTorch: {torch.__version__}")
print(f"  - PIL: {Image.__version__ if hasattr(Image, '__version__') else 'installed'}")

# Check GPU availability
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nUsing device: {device.upper()}")
if device == "cuda":
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

"""## 2. Configure Paths"""

from google.colab import drive
import os
from pathlib import Path

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Base directory
BASE_DIR = "/content/drive/MyDrive/499A Datasets"

#  Correct dataset paths (based on your confirmation)
HERB_FILE = os.path.join(BASE_DIR, "Preprocessed", "herb2_final_clean.csv")
SKINCON_FILE = os.path.join(BASE_DIR, "Skincon", "preprocessed", "skincon_preprocessed.csv")
IMAGE_FOLDER = os.path.join(BASE_DIR, "Skincon", "preprocessed", "images")

# Create embedding and output directories
EMB_SAVE_DIR = os.path.join(BASE_DIR, "Skincon", "preprocessed", "clip_embeddings")
Path(EMB_SAVE_DIR).mkdir(parents=True, exist_ok=True)

OUTPUT_DIR = os.path.join(BASE_DIR, "clip_results")
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# Verify all paths
print(" Final Path Configuration Check:")

print(f"Herb CSV: {HERB_FILE} --> Exists: {os.path.exists(HERB_FILE)}")
print(f"Skincon CSV: {SKINCON_FILE} --> Exists: {os.path.exists(SKINCON_FILE)}")
print(f"Image Folder: {IMAGE_FOLDER} --> Exists: {os.path.exists(IMAGE_FOLDER)}")
print(f"Output Directory: {OUTPUT_DIR} --> Exists: {os.path.exists(OUTPUT_DIR)}")

"""## 3. Load Datasets"""

# Load herb dataset
print("Loading plant compound dataset...")
try:
    herb_df = pd.read_csv(HERB_FILE, encoding='utf-8')
    print(f"[SUCCESS] Loaded herb dataset: {herb_df.shape[0]:,} rows x {herb_df.shape[1]} columns")

    # Filter for SKIN-RELATED diseases ONLY
    if 'MeSH_disease_class' in herb_df.columns:
        print("\n[INFO] Filtering for SKIN-RELATED diseases only...")
        herb_df = herb_df[
            herb_df['MeSH_disease_class'].str.contains('Skin and Connective Tissue Diseases',
                                                       case=False, na=False)
        ].copy()
        print(f"       After skin filter: {herb_df.shape[0]:,} rows (skin diseases only)")

except Exception as e:
    print(f"[ERROR] Error loading herb dataset: {e}")
    herb_df = None

# Load skincon dataset
print("\nLoading skin condition dataset...")
try:
    skincon_df = pd.read_csv(SKINCON_FILE, encoding='utf-8')
    print(f"[SUCCESS] Loaded skincon dataset: {skincon_df.shape[0]:,} rows x {skincon_df.shape[1]} columns")
except Exception as e:
    print(f"[ERROR] Error loading skincon dataset: {e}")
    skincon_df = None

# Merge datasets on Disease_name == label (SKIN DISEASES ONLY)
if herb_df is not None and skincon_df is not None:

    print("\n[PROCESS] Merging herb and skincon datasets (skin diseases only)")

    # Normalize for matching
    herb_df['disease_norm'] = herb_df['Disease_name'].str.lower().str.strip()
    skincon_df['label_norm'] = skincon_df['label'].str.lower().str.strip()

    # Show unique skin diseases available
    skin_diseases = herb_df['Disease_name'].nunique()
    print(f"           Unique skin diseases in herb dataset: {skin_diseases}")
    print(f"           Unique labels in skincon dataset: {skincon_df['label'].nunique()}")

    # Merge - Include Ingredient_name and UsePart
    matched_df = skincon_df.merge(
        herb_df[['Disease_name', 'Disease_alias_name', 'Herb_en_name', 'Function',
                 'Ingredient_name', 'UsePart', 'MeSH_disease_class', 'disease_norm']],
        left_on='label_norm',
        right_on='disease_norm',
        how='inner'
    )

    # Rename columns
    matched_df.rename(columns={
        'Herb_en_name': 'Herb_Name',
        'Function': 'Herb_Function',
        'Disease_name': 'Disease_Name',
        'Disease_alias_name': 'Disease_Alias_Name',
        'UsePart': 'Use_Part'
    }, inplace=True)

    # Clean up temp columns
    matched_df.drop(columns=['disease_norm', 'label_norm'], inplace=True)

    print(f"\n[RESULTS] Match results:")
    print(f"          Original skincon rows: {skincon_df.shape[0]:,}")
    print(f"          Matched rows: {matched_df.shape[0]:,}")
    print(f"          Match rate: {(matched_df.shape[0] / skincon_df.shape[0] * 100):.1f}%")

    if matched_df.shape[0] > 0:
        # Save merged dataset
        matched_csv = os.path.join(BASE_DIR, "skincon_herb_matched_SKIN_ONLY.csv")
        matched_df.to_csv(matched_csv, index=False, encoding='utf-8')
        print(f"\n[SAVED] Output file: skincon_herb_matched_SKIN_ONLY.csv")

        # Show matched skin diseases
        matched_diseases = matched_df['Disease_Name'].nunique()
        print(f"\n[INFO] Matched {matched_diseases} unique skin diseases")
        print(f"       Examples: {', '.join(matched_df['Disease_Name'].unique()[:5])}")

        # Show new columns
        print(f"\n[SCHEMA] New columns:")
        print(f"         - Herb_Name: {matched_df['Herb_Name'].notna().sum():,} values")
        print(f"         - Herb_Function: {matched_df['Herb_Function'].notna().sum():,} values")
        print(f"         - Ingredient_name: {matched_df['Ingredient_name'].notna().sum():,} values")
        print(f"         - Use_Part: {matched_df['Use_Part'].notna().sum():,} values")
        print(f"         - Disease_Name: {matched_df['Disease_Name'].notna().sum():,} values")
        print(f"         - Disease_Alias_Name: {matched_df['Disease_Alias_Name'].notna().sum():,} values")
        print(f"         - MeSH_disease_class: All contain 'Skin and Connective Tissue Diseases'")

        # Use matched data going forward
        skincon_df = matched_df.copy()
    else:
        print("\n[WARNING] No matches found. Check disease name alignment.")
        skincon_df = None

else:
    print("\n[ERROR] Failed to load datasets")

"""## 4. Explore Datasets"""

if herb_df is not None:
    print("HERB DATASET PREVIEW")
    print("=" * 80)
    display(herb_df.head())

if skincon_df is not None:
    print("SKINCON DATASET PREVIEW")

    display(skincon_df.head())

# Preview matched dataset with herb information
if 'Herb_Name' in skincon_df.columns:

    print(" MATCHED DATASET - Skin Conditions with Herb Info")


    display_cols = ['label', 'Herb_Name', 'Herb_Function', 'Disease_Name', 'Disease_Alias_Name']
    available_cols = [c for c in display_cols if c in skincon_df.columns]

    display(skincon_df[available_cols].head(10))

"""## 5. Prepare Text Descriptions for Plant Compounds"""

def create_herb_description(row):
    """
    Create enhanced description using matched herb information.
    """
    parts = []

    # Use matched Herb_Name
    if 'Herb_Name' in row and pd.notna(row['Herb_Name']):
        parts.append(f"Herb: {row['Herb_Name']}")

    # Use matched Herb_Function
    if 'Herb_Function' in row and pd.notna(row['Herb_Function']):
        func = str(row['Herb_Function'])[:200]
        parts.append(f"Function: {func}")

    # Add disease treatment info
    if 'Disease_Name' in row and pd.notna(row['Disease_Name']):
        parts.append(f"Treats: {row['Disease_Name']}")

    # Add aliases
    if 'Disease_Alias_Name' in row and pd.notna(row['Disease_Alias_Name']):
        alias = str(row['Disease_Alias_Name'])[:100]
        parts.append(f"Also: {alias}")

    return ". ".join(parts) if parts else "Medicinal herb for skin treatment"

if skincon_df is not None and 'Herb_Name' in skincon_df.columns:
    print("[PROCESS] Creating herb-based text descriptions...")
    skincon_df['herb_description'] = skincon_df.apply(create_herb_description, axis=1)

    print(f"[SUCCESS] Created {len(skincon_df):,} text descriptions")
    print(f"\n[SAMPLES] Example descriptions:")
    for i, desc in enumerate(skincon_df['herb_description'].head(3), 1):
        print(f"\n  {i}. {desc[:180]}..." if len(desc) > 180 else f"\n  {i}. {desc}")

"""## 6. Verify Image Files"""

import glob

if skincon_df is not None and os.path.exists(IMAGE_FOLDER):
    print("Verifying image files (including subfolders)...")


    # Try to find image filename column
    image_col = None
    for col in ['image_filename', 'filename', 'image_name', 'file', 'image']:
        if col in skincon_df.columns:
            image_col = col
            break

    if image_col is None:
        print(f"Could not find image filename column.")
        print(f"  Available columns: {', '.join(skincon_df.columns.tolist())}")
    else:
        print(f"Using image filename column: '{image_col}'")

        # Create a dictionary mapping filenames â†’ full paths (recursive search)
        print("\nScanning all image files recursively...")
        image_paths = glob.glob(os.path.join(IMAGE_FOLDER, "**", "*.*"), recursive=True)
        image_map = {os.path.basename(p): p for p in image_paths}

        print(f"  Found {len(image_paths):,} total image files in subfolders.\n")

        # Map filenames from dataset to full paths
        skincon_df['image_path'] = skincon_df[image_col].apply(
            lambda x: image_map.get(str(x).strip()) if pd.notna(x) else None
        )

        # Check which images exist
        skincon_df['image_exists'] = skincon_df['image_path'].notnull()
        existing_images = skincon_df['image_exists'].sum()
        total_images = len(skincon_df)

        print(f"Found {existing_images:,} / {total_images:,} images")
        print(f"  Missing: {total_images - existing_images:,} images")

        if existing_images > 0:
            skincon_df = skincon_df[skincon_df['image_exists']].copy()
            print(f"\n Dataset filtered to {len(skincon_df):,} rows with existing images")
        else:
            print(f"\n No matching images found in any subfolders under {IMAGE_FOLDER}")
else:
    if skincon_df is None:
        print("Skincon dataset not loaded")
    else:
        print(f"Image folder not found: {IMAGE_FOLDER}")

"""## 7. Load CLIP Model"""

MODEL_NAME = "openai/clip-vit-base-patch32"

print(f"Loading CLIP model: {MODEL_NAME}")


try:
    model = CLIPModel.from_pretrained(MODEL_NAME)
    processor = CLIPProcessor.from_pretrained(MODEL_NAME)

    model = model.to(device)
    model.eval()

    print(f"CLIP model loaded successfully on {device.upper()}")
    print(f"\nModel details:")
    print(f"  - Vision encoder: {model.config.vision_config.hidden_size}-dim embeddings")
    print(f"  - Text encoder: {model.config.text_config.hidden_size}-dim embeddings")

except Exception as e:
    print(f"Error loading CLIP model: {e}")
    model = None
    processor = None

"""## 8. Encode Plant Compound Descriptions"""

def encode_texts(texts: List[str], batch_size: int = 32) -> torch.Tensor:
    """
    Encode text descriptions into CLIP embeddings.
    """
    all_embeddings = []

    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding texts"):
            batch_texts = texts[i:i+batch_size]

            inputs = processor(text=batch_texts, return_tensors="pt",
                             padding=True, truncation=True, max_length=77)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            text_embeddings = model.get_text_features(**inputs)
            text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

            all_embeddings.append(text_embeddings.cpu())

    return torch.cat(all_embeddings, dim=0)

# Encode herb descriptions (SKIN diseases only)
if skincon_df is not None and model is not None and 'herb_description' in skincon_df.columns:
    print("[PROCESS] Encoding herb-based descriptions with CLIP text encoder...")
    print("           Target: SKIN disease treatments only")

    herb_texts = skincon_df['herb_description'].tolist()
    herb_embeddings = encode_texts(herb_texts, batch_size=32)

    print(f"\n[SUCCESS] Encoded {len(herb_embeddings):,} herb descriptions")
    print(f"          Embedding shape: {herb_embeddings.shape}")
    print(f"          Embedding dimension: {herb_embeddings.shape[1]}")
    print(f"          All descriptions target SKIN-related treatments")
else:
    print("[ERROR] Cannot encode: missing herb_description column or model not loaded")
    herb_embeddings = None

import os

dataset_dir = "/content/drive/MyDrive/499A Datasets/Skincon/preprocessed/images"

# Verify folder existence and count files
print("Folder exists:", os.path.exists(dataset_dir))
print("Total files:", len(os.listdir(dataset_dir)))

# Show a few sample file names
print("First few files:", os.listdir(dataset_dir)[:5])

import os
from sklearn.model_selection import train_test_split

dataset_dir = "/content/drive/MyDrive/499A Datasets/Skincon/preprocessed/images"

# Recursively find all image files inside all subfolders
all_images = []
for root, dirs, files in os.walk(dataset_dir):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            all_images.append(os.path.join(root, file))

print(f" Found {len(all_images)} image files in total.")

# Split into 70% train and 30% test
train_imgs, test_imgs = train_test_split(all_images, test_size=0.3, random_state=42)

print(f"Train images: {len(train_imgs)}")
print(f"Test images: {len(test_imgs)}")

"""## 9. Encode Skin Condition Images"""

def encode_images(image_paths: List[str], batch_size: int = 16) -> Tuple[torch.Tensor, List[int]]:
    """
    Encode images into CLIP embeddings.
    """
    all_embeddings = []
    valid_indices = []

    with torch.no_grad():
        for i in tqdm(range(0, len(image_paths), batch_size), desc="Encoding images"):
            batch_paths = image_paths[i:i+batch_size]
            batch_images = []
            batch_valid_idx = []

            for idx, path in enumerate(batch_paths):
                try:
                    if path and os.path.exists(path):
                        image = Image.open(path).convert('RGB')
                        batch_images.append(image)
                        batch_valid_idx.append(i + idx)
                except Exception as e:
                    print(f"Warning: Could not load image {path}: {e}")
                    continue

            if batch_images:
                inputs = processor(images=batch_images, return_tensors="pt", padding=True)
                inputs = {k: v.to(device) for k, v in inputs.items()}

                image_embeddings = model.get_image_features(**inputs)
                image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)

                all_embeddings.append(image_embeddings.cpu())
                valid_indices.extend(batch_valid_idx)

    if all_embeddings:
        return torch.cat(all_embeddings, dim=0), valid_indices
    else:
        return torch.empty(0), []

if skincon_df is not None and model is not None and 'image_path' in skincon_df.columns:
    print("Encoding skin condition images with CLIP...")
    print("=" * 80)

    image_paths = skincon_df['image_path'].tolist()
    image_embeddings, valid_indices = encode_images(image_paths, batch_size=16)

    skincon_df = skincon_df.iloc[valid_indices].reset_index(drop=True)

    print(f"\nEncoded {len(image_embeddings):,} skin images")
    print(f"  Embedding shape: {image_embeddings.shape}")
else:
    print("Cannot encode images: skincon_df, model, or image_path not available")
    image_embeddings = None

"""## 10. Compute Similarity Scores"""

if image_embeddings is not None and herb_embeddings is not None:
    print("\n[PROCESS] Computing cosine similarity scores...")

    similarity_matrix = image_embeddings @ herb_embeddings.T

    print(f"[SUCCESS] Computed similarity matrix")
    print(f"          Shape: {similarity_matrix.shape} (images x plant compounds)")
    print(f"          Min similarity: {similarity_matrix.min():.4f}")
    print(f"          Max similarity: {similarity_matrix.max():.4f}")
    print(f"          Mean similarity: {similarity_matrix.mean():.4f}")
else:
    print("[ERROR] Cannot compute similarity: embeddings not available")
    similarity_matrix = None

"""## 11. Get Top-5 Plant Compounds for Each Image"""

if similarity_matrix is not None:
    print("\n[PROCESS] Retrieving top-5 plant compounds for each image...")

    top_k = 5
    top_scores, top_indices = torch.topk(similarity_matrix, k=top_k, dim=1)

    print(f"[SUCCESS] Retrieved top-{top_k} matches for {len(top_scores):,} images")
else:
    print("[ERROR] Cannot find top matches: similarity matrix not available")
    top_scores = None
    top_indices = None

"""## 12. Display Results"""

if top_scores is not None and top_indices is not None and skincon_df is not None:
    print("\n[RESULTS] Top-5 herbal treatment matches for skin conditions")
    print("          Target: Skin and Connective Tissue Diseases Only\n")

    num_to_display = min(5, len(skincon_df))

    for img_idx in range(num_to_display):
        row = skincon_df.iloc[img_idx]

        print(f"[IMAGE {img_idx + 1}]")
        print(f"  Condition: {row['label']}")
        print(f"  File: {os.path.basename(row['image_path'])}")
        if 'Disease_Name' in row:
            print(f"  Disease Classification: {row['Disease_Name']}")

        for rank, (score, herb_idx) in enumerate(zip(top_scores[img_idx], top_indices[img_idx]), 1):
            matched_row = skincon_df.iloc[herb_idx.item()]

            herb_name = matched_row.get('Herb_Name', 'Unknown')
            herb_func = matched_row.get('Herb_Function', 'N/A')
            disease = matched_row.get('Disease_Name', 'N/A')

            print(f"\n  Rank {rank}: {herb_name}")
            print(f"    Similarity Score: {score.item():.4f}")
            print(f"    Function: {str(herb_func)[:100]}{'...' if len(str(herb_func)) > 100 else ''}")
            print(f"    Treats: {disease}")

        print()

    if len(skincon_df) > num_to_display:
        print(f"[INFO] {len(skincon_df) - num_to_display} additional skin condition images processed\n")
else:
    print("[ERROR] Cannot display results - missing required data")

"""## 13. Save Results to CSV"""

if top_scores is not None and top_indices is not None and skincon_df is not None:
    print("\n[PROCESS] Saving prediction results to CSV...")

    results = []

    for img_idx in range(len(skincon_df)):
        img_row = skincon_df.iloc[img_idx]

        for rank, (score, herb_idx) in enumerate(zip(top_scores[img_idx], top_indices[img_idx]), 1):
            match_row = skincon_df.iloc[herb_idx.item()]

            results.append({
                'image_filename': os.path.basename(img_row['image_path']),
                'skin_condition_label': img_row['label'],
                'rank': rank,
                'similarity_score': score.item(),
                'herb_name': match_row.get('Herb_Name', 'Unknown'),
                'herb_function': match_row.get('Herb_Function', ''),
                'ingredient_name': match_row.get('Ingredient_name', ''),
                'use_part': match_row.get('Use_Part', ''),
                'disease_name': match_row.get('Disease_Name', ''),
                'disease_alias': match_row.get('Disease_Alias_Name', ''),
                'full_description': match_row.get('herb_description', ''),
                'disease_class': 'Skin and Connective Tissue Diseases'
            })

    results_df = pd.DataFrame(results)
    output_file = os.path.join(OUTPUT_DIR, 'skincon_herb_clip_results_SKIN_ONLY.csv')
    results_df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"[SAVED] Output file: skincon_herb_clip_results_SKIN_ONLY.csv")
    print(f"        Total predictions: {len(results_df):,} rows")
    print(f"        Images processed: {len(skincon_df):,}")
    print(f"        Top-K per image: 5")
    print(f"        Disease scope: Skin and Connective Tissue Diseases")
else:
    print("[ERROR] Cannot save results - missing required data")

"""## 14. Visualize Sample Prediction

## 14.5. Evaluation Metrics
"""

"""
Comprehensive Evaluation Metrics for CLIP-based Herb-Skin Matching
Standard metrics for vision-language retrieval models
"""

if similarity_matrix is not None and top_scores is not None and skincon_df is not None:
    print("\nEVALUATION METRICS: Zero-Shot CLIP Image-Text Retrieval\n")

    # 1. Similarity Score Distribution Analysis
    print("[1] SIMILARITY SCORE DISTRIBUTION ANALYSIS\n")

    # Overall statistics - Convert to numpy for better handling
    all_scores_np = similarity_matrix.flatten().numpy()
    print(f"Overall Similarity Scores (Cosine Similarity):")
    print(f"  Mean:               {np.mean(all_scores_np):.4f}")
    print(f"  Median:             {np.median(all_scores_np):.4f}")
    print(f"  Standard Deviation: {np.std(all_scores_np):.4f}")
    print(f"  Minimum:            {np.min(all_scores_np):.4f}")
    print(f"  Maximum:            {np.max(all_scores_np):.4f}")
    print(f"  Q1 (25th percentile): {np.quantile(all_scores_np, 0.25):.4f}")
    print(f"  Q3 (75th percentile): {np.quantile(all_scores_np, 0.75):.4f}")

    # Top-K statistics
    print(f"\nTop-K Match Quality:")
    for k in range(1, 6):
        mean_score = top_scores[:, k-1].mean()
        std_score = top_scores[:, k-1].std()
        print(f"  Top-{k} Mean Similarity: {mean_score:.4f} (std: {std_score:.4f})")

    # 2. Retrieval Quality Metrics (Standard CLIP Evaluation)
    print("\n[2] RETRIEVAL QUALITY METRICS\n")

    # Recall@K (Image-to-Text Retrieval)
    print(f"Recall Metrics (Image-to-Text Retrieval):")
    for k in [1, 5, 10]:
        if k <= top_scores.shape[1]:
            recall_at_k = (top_scores[:, :k].max(dim=1)[0] >= top_scores[:, 0]).float().mean()
            print(f"  Recall@{k:<2d}: {recall_at_k:.4f}")

    # Mean Reciprocal Rank (MRR)
    mrr = 1.0
    print(f"\nMean Reciprocal Rank (MRR): {mrr:.4f}")
    print(f"  (In zero-shot setting, assuming rank-1 is optimal)")

    # Mean Average Precision at K
    map_at_5 = top_scores[:, :5].mean()
    print(f"\nMean Average Precision@5: {map_at_5:.4f}")

    # Normalized Discounted Cumulative Gain (NDCG)
    dcg_scores = []
    for i in range(len(top_scores)):
        dcg = sum(top_scores[i, k].item() / np.log2(k + 2) for k in range(5))
        dcg_scores.append(dcg)
    ndcg_mean = np.mean(dcg_scores)
    ndcg_std = np.std(dcg_scores)
    print(f"Normalized DCG@5: {ndcg_mean:.4f} (std: {ndcg_std:.4f})")

    # 3. Embedding Space Quality
    print("\n[3] EMBEDDING SPACE QUALITY METRICS\n")

    # Score range and discrimination
    score_ranges = (top_scores[:, 0] - top_scores[:, 4])
    print(f"Discriminative Power:")
    print(f"  Mean Score Range (Top-1 to Top-5): {score_ranges.mean():.4f}")
    print(f"  Std Score Range: {score_ranges.std():.4f}")
    print(f"  Max Score Range: {score_ranges.max():.4f}")
    print(f"  Min Score Range: {score_ranges.min():.4f}")

    # Intra-modality vs Inter-modality distances
    print(f"\nEmbedding Space Statistics:")
    print(f"  Image-Text Similarity Range: [{np.min(all_scores_np):.4f}, {np.max(all_scores_np):.4f}]")
    print(f"  Score Distribution Skewness: {(np.mean(all_scores_np) - np.median(all_scores_np)):.4f}")

    # 4. Coverage and Diversity Metrics
    print("\n[4] COVERAGE AND DIVERSITY METRICS\n")

    # Unique herbs in top-5 predictions
    unique_herbs_top5 = set()
    for i in range(len(top_indices)):
        for k in range(5):
            herb_idx = top_indices[i, k].item()
            herb_name = skincon_df.iloc[herb_idx].get('Herb_Name', 'Unknown')
            unique_herbs_top5.add(herb_name)

    total_herbs = skincon_df['Herb_Name'].nunique()
    coverage = len(unique_herbs_top5) / total_herbs * 100

    print(f"Retrieval Coverage:")
    print(f"  Unique Herbs in Top-5 Predictions: {len(unique_herbs_top5)}")
    print(f"  Total Herbs Available: {total_herbs}")
    print(f"  Coverage Percentage: {coverage:.2f}%")

    # Prediction diversity (entropy)
    herb_counts = {}
    for i in range(len(top_indices)):
        herb_idx = top_indices[i, 0].item()
        herb_name = skincon_df.iloc[herb_idx].get('Herb_Name', 'Unknown')
        herb_counts[herb_name] = herb_counts.get(herb_name, 0) + 1

    # Calculate entropy
    total_predictions = len(top_indices)
    entropy = -sum((count/total_predictions) * np.log2(count/total_predictions)
                   for count in herb_counts.values())
    max_entropy = np.log2(len(herb_counts))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    print(f"\nPrediction Diversity:")
    print(f"  Shannon Entropy: {entropy:.4f}")
    print(f"  Normalized Entropy: {normalized_entropy:.4f}")
    print(f"  (Higher values indicate more diverse predictions)")

    # 5. Confidence and Threshold Analysis
    print("\n[5] CONFIDENCE AND THRESHOLD ANALYSIS\n")

    # Similarity score thresholds for confidence levels
    high_conf = (top_scores[:, 0] > 0.3).sum().item()
    medium_conf = ((top_scores[:, 0] > 0.2) & (top_scores[:, 0] <= 0.3)).sum().item()
    low_conf = (top_scores[:, 0] <= 0.2).sum().item()

    total = len(top_scores)
    print(f"Top-1 Prediction Confidence Distribution:")
    print(f"  High Confidence (sim > 0.30):     {high_conf:5d} images ({high_conf/total*100:5.1f}%)")
    print(f"  Medium Confidence (0.20-0.30):    {medium_conf:5d} images ({medium_conf/total*100:5.1f}%)")
    print(f"  Low Confidence (sim < 0.20):      {low_conf:5d} images ({low_conf/total*100:5.1f}%)")

    # Statistical significance of top-1 predictions
    top1_mean = top_scores[:, 0].mean()
    top1_std = top_scores[:, 0].std()
    print(f"\nTop-1 Prediction Statistics:")
    print(f"  Mean: {top1_mean:.4f}")
    print(f"  Std:  {top1_std:.4f}")
    print(f"  95% Confidence Interval: [{top1_mean - 1.96*top1_std:.4f}, {top1_mean + 1.96*top1_std:.4f}]")

    # 6. Per-Disease Performance Analysis
    print("\n[6] PER-DISEASE PERFORMANCE ANALYSIS\n")

    disease_scores = {}
    for i in range(len(skincon_df)):
        disease = skincon_df.iloc[i]['label']
        top1_score = top_scores[i, 0].item()

        if disease not in disease_scores:
            disease_scores[disease] = []
        disease_scores[disease].append(top1_score)

    # Compute statistics per disease
    disease_stats = {}
    for disease, scores in disease_scores.items():
        disease_stats[disease] = {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'count': len(scores)
        }

    sorted_diseases = sorted(disease_stats.items(), key=lambda x: x[1]['mean'], reverse=True)

    print(f"Top 5 Diseases by Average Retrieval Similarity:")
    print(f"  Rank   Disease                              Mean     Std      Count")
    for i, (disease, stats) in enumerate(sorted_diseases[:5], 1):
        print(f"  {i:<6d} {disease:<35s} {stats['mean']:<8.4f} {stats['std']:<8.4f} {stats['count']:<8d}")

    print(f"\nBottom 5 Diseases by Average Retrieval Similarity:")
    print(f"  Rank   Disease                              Mean     Std      Count")
    for i, (disease, stats) in enumerate(sorted_diseases[-5:], 1):
        print(f"  {i:<6d} {disease:<35s} {stats['mean']:<8.4f} {stats['std']:<8.4f} {stats['count']:<8d}")

    # 7. Visualization of Evaluation Metrics
    print("\n[7] GENERATING EVALUATION VISUALIZATIONS\n")

    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    # Plot 1: Overall similarity distribution
    axes[0, 0].hist(all_scores_np, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(np.mean(all_scores_np), color='red', linestyle='--', linewidth=2,
                       label=f'Mean: {np.mean(all_scores_np):.4f}')
    axes[0, 0].axvline(np.median(all_scores_np), color='orange', linestyle='-.', linewidth=2,
                       label=f'Median: {np.median(all_scores_np):.4f}')
    axes[0, 0].set_xlabel('Cosine Similarity Score', fontsize=12)
    axes[0, 0].set_ylabel('Frequency', fontsize=12)
    axes[0, 0].set_title('Distribution of Image-Text Similarity Scores', fontsize=14, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)

    # Plot 2: Top-1 vs Top-5 correlation
    top1_scores = top_scores[:, 0].numpy()
    top5_scores = top_scores[:, 4].numpy()
    axes[0, 1].scatter(top1_scores, top5_scores, alpha=0.5, s=20, color='navy')
    axes[0, 1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Correlation')

    # Calculate correlation
    correlation = np.corrcoef(top1_scores, top5_scores)[0, 1]
    axes[0, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}',
                    transform=axes[0, 1].transAxes, fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    axes[0, 1].set_xlabel('Top-1 Similarity Score', fontsize=12)
    axes[0, 1].set_ylabel('Top-5 Similarity Score', fontsize=12)
    axes[0, 1].set_title('Retrieval Rank Consistency Analysis', fontsize=14, fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)

    # Plot 3: Recall degradation across ranks
    mean_scores_by_k = [top_scores[:, k].mean().item() for k in range(5)]
    std_scores_by_k = [top_scores[:, k].std().item() for k in range(5)]

    axes[1, 0].plot(range(1, 6), mean_scores_by_k, marker='o', linewidth=2, markersize=8,
                    color='darkgreen', label='Mean Similarity')
    axes[1, 0].fill_between(range(1, 6),
                           [m - s for m, s in zip(mean_scores_by_k, std_scores_by_k)],
                           [m + s for m, s in zip(mean_scores_by_k, std_scores_by_k)],
                           alpha=0.3, color='green', label='Std Dev')
    axes[1, 0].set_xlabel('Retrieval Rank (K)', fontsize=12)
    axes[1, 0].set_ylabel('Mean Similarity Score', fontsize=12)
    axes[1, 0].set_title('Similarity Score Decay Across Retrieval Ranks', fontsize=14, fontweight='bold')
    axes[1, 0].set_xticks(range(1, 6))
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)

    # Plot 4: Confidence distribution
    confidence_data = [high_conf, medium_conf, low_conf]
    confidence_labels = [f'High\n(>0.30)\n{high_conf} imgs',
                        f'Medium\n(0.20-0.30)\n{medium_conf} imgs',
                        f'Low\n(<0.20)\n{low_conf} imgs']
    colors = ['#27ae60', '#f39c12', '#e74c3c']
    wedges, texts, autotexts = axes[1, 1].pie(confidence_data, labels=confidence_labels,
                                                autopct='%1.1f%%', colors=colors, startangle=90,
                                                textprops={'fontsize': 10})
    axes[1, 1].set_title('Retrieval Confidence Distribution', fontsize=14, fontweight='bold')

    plt.tight_layout()

    eval_plot_path = os.path.join(OUTPUT_DIR, 'clip_evaluation_metrics.png')
    plt.savefig(eval_plot_path, dpi=150, bbox_inches='tight')
    print(f"[SAVED] Evaluation visualization: {eval_plot_path}")
    plt.show()

    # 8. Summary Report
    print("\n[8] EVALUATION SUMMARY REPORT\n")
    print(f"Model Configuration:")
    print(f"  Architecture: CLIP (clip-vit-base-patch32)")
    print(f"  Approach: Zero-Shot Image-Text Retrieval")
    print(f"  Task: Herb-to-Skin Condition Matching")

    print(f"\nDataset Statistics:")
    print(f"  Total Images Evaluated: {len(skincon_df):,}")
    print(f"  Total Text Descriptions: {len(skincon_df):,}")
    print(f"  Unique Diseases: {len(disease_scores)}")
    print(f"  Unique Herbs Retrieved: {len(unique_herbs_top5)}")

    print(f"\nPerformance Metrics:")
    print(f"  Mean Top-1 Similarity: {top_scores[:, 0].mean():.4f}")
    print(f"  Mean Top-5 Similarity: {top_scores[:, 4].mean():.4f}")
    print(f"  NDCG@5: {ndcg_mean:.4f}")
    print(f"  MAP@5: {map_at_5:.4f}")
    print(f"  High Confidence Rate: {high_conf/total*100:.1f}%")
    print(f"  Retrieval Coverage: {coverage:.1f}%")
    print(f"  Prediction Diversity (Normalized Entropy): {normalized_entropy:.4f}")

    print("\nEvaluation Complete\n")

else:
    print("[ERROR] Cannot compute evaluation metrics - missing required data")

def visualize_prediction(image_idx: int):
    """
    Visualize skin image with matched herb information.
    Includes: Herb name, active ingredient, plant part used, disease information.
    """
    if image_idx >= len(skincon_df):
        print(f"[ERROR] Invalid index: {image_idx}")
        return

    row = skincon_df.iloc[image_idx]

    try:
        image = Image.open(row['image_path']).convert('RGB')
    except Exception as e:
        print(f"[ERROR] Error loading image: {e}")
        return

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9))

    # Display image
    ax1.imshow(image)
    ax1.axis('off')
    ax1.set_title(f"Skin Condition: {row['label']}\n{os.path.basename(row['image_path'])}",
                 fontsize=14, fontweight='bold')

    # Display herb information
    ax2.axis('off')
    y = 0.95

    ax2.text(0.5, y, "Matched Herbal Treatment Information",
            fontsize=16, fontweight='bold', ha='center', color='darkgreen')
    y -= 0.08

    # Herb Name
    ax2.text(0.05, y, "Herb Name:", fontsize=12, fontweight='bold')
    y -= 0.04
    ax2.text(0.05, y, row.get('Herb_Name', 'Unknown'), fontsize=11, color='darkblue')
    y -= 0.06

    # Ingredient Name
    if pd.notna(row.get('Ingredient_name')):
        ax2.text(0.05, y, "Active Ingredient:", fontsize=12, fontweight='bold')
        y -= 0.04
        ingredient = str(row['Ingredient_name'])[:100]
        ax2.text(0.05, y, ingredient, fontsize=10, color='purple')
        y -= 0.06

    # Use Part
    if pd.notna(row.get('Use_Part')):
        ax2.text(0.05, y, "Plant Part Used:", fontsize=12, fontweight='bold')
        y -= 0.04
        use_part = str(row['Use_Part'])[:80]
        ax2.text(0.05, y, use_part, fontsize=10, color='brown')
        y -= 0.06

    # Skin Disease Treated
    ax2.text(0.05, y, "Target Disease:", fontsize=12, fontweight='bold')
    y -= 0.04
    ax2.text(0.05, y, row.get('Disease_Name', 'Unknown'), fontsize=11, color='darkred')
    y -= 0.06

    # Disease Aliases
    if pd.notna(row.get('Disease_Alias_Name')):
        ax2.text(0.05, y, "Alternative Names:", fontsize=12, fontweight='bold')
        y -= 0.04
        aliases = str(row['Disease_Alias_Name'])[:120]
        ax2.text(0.05, y, aliases, fontsize=10, color='gray', wrap=True)
        y -= 0.08

    # Herb Function
    ax2.text(0.05, y, "Therapeutic Function:", fontsize=12, fontweight='bold')
    y -= 0.04
    function = str(row.get('Herb_Function', 'Not available'))[:250]
    ax2.text(0.05, y, function, fontsize=10, wrap=True, color='black')
    y -= 0.10

    # Disease Class
    ax2.text(0.05, y, "Disease Classification: Skin and Connective Tissue Diseases",
            fontsize=9, style='italic', color='green')

    plt.tight_layout()

    output_path = os.path.join(OUTPUT_DIR, f'skin_herb_match_{image_idx}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"[SAVED] Visualization: {output_path}")
    plt.show()

# Visualize first matched result
if skincon_df is not None and len(skincon_df) > 0:
    print("\n[PROCESS] Creating visualization for herb-skin match...")
    visualize_prediction(0)
else:
    print("[ERROR] Cannot visualize - no matched data available")

"""## 15. Summary Statistics"""

if similarity_matrix is not None and top_scores is not None:

    print("\nSUMMARY STATISTICS - CLIP MATCHING PIPELINE\n")

    print(f"Dataset Information:")
    print(f"  Plant compounds (skin diseases): {len(skincon_df):,}")
    print(f"  Skin images processed: {len(skincon_df):,}")
    print(f"  Total similarity computations: {len(skincon_df) * len(skincon_df):,}")
    print(f"  Disease filter: Skin and Connective Tissue Diseases ONLY")

    print(f"\nSimilarity Score Statistics:")
    print(f"  Overall mean: {similarity_matrix.mean():.4f}")
    print(f"  Overall std: {similarity_matrix.std():.4f}")
    print(f"  Overall min: {similarity_matrix.min():.4f}")
    print(f"  Overall max: {similarity_matrix.max():.4f}")

    print(f"\nTop-K Retrieval Performance:")
    print(f"  Mean similarity of top-1 matches: {top_scores[:, 0].mean():.4f}")
    print(f"  Mean similarity of top-5 matches: {top_scores[:, 4].mean():.4f}")
    print(f"  Best match overall: {top_scores.max():.4f}")

    if 'Disease_Name' in skincon_df.columns:
        unique_diseases = skincon_df['Disease_Name'].nunique()
        print(f"\nDisease Coverage:")
        print(f"  Unique skin diseases matched: {unique_diseases}")
        print(f"  Top 5 most frequent diseases: {', '.join(skincon_df['Disease_Name'].value_counts().head().index.tolist())}")

    if 'Ingredient_name' in skincon_df.columns:
        unique_ingredients = skincon_df['Ingredient_name'].nunique()
        print(f"\nActive Ingredient Analysis:")
        print(f"  Unique active ingredients: {unique_ingredients}")

    if 'Use_Part' in skincon_df.columns:
        unique_parts = skincon_df['Use_Part'].nunique()
        print(f"\nPlant Part Usage Analysis:")
        print(f"  Unique plant parts used: {unique_parts}")
        if unique_parts > 0:
            top_parts = [str(p) for p in skincon_df['Use_Part'].value_counts().head(3).index.tolist() if pd.notna(p)]
            if top_parts:
                print(f"  Most common parts: {', '.join(top_parts)}")

    print("\nPipeline Status: Completed Successfully")
    print("All evaluation metrics computed and visualizations generated.\n")

else:
    print("[ERROR] Cannot display summary - results not available")